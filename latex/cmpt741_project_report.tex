\documentclass[11pt]{article}

% pretty tables
\usepackage{booktabs}

% Math symbols and functions
\usepackage{amsmath}

% For using Float barriers
\usepackage{placeins}

% images!
\usepackage{graphicx}

% Writing algotihms
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}

% For drawing
\usepackage{tikz}

\usepackage{csquotes}

\usepackage{multicol}

\usepackage[T1]{fontenc}
\usepackage{titling}

\usepackage[margin=1.5cm]{geometry}

\usepackage{url}

% Subsections will be lettered instead of numbered
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\sloppy
\clubpenalty=10000  % Para evitar linhas orfas
\widowpenalty=10000 % Para evitar linhas viuvas
\hyphenpenalty=10000 % Para n√£o hifenizar

\title{CMPT 741 - Data Mining\\ Project Report}
\author{Walther Maciel - 301278740\\ Course Based M.Sc.}

\begin{document}
\maketitle

%\begin{multicols}{2}

\section{Introduction}
For a proper introduction please refer to the assignment question document available at: www.com

\section{Methodology}
The Methodology for providing recommendations to which words are missing from the documents consists of three basic steps: clustering, frequent pattern mining and association rule matching.


\subsection{Clustering}
The first step in the whole process is the partitioning of the dataset into 4 clusters. Each one of them corresponds to one of the four mini assignments used to create the dataset.

It is assumed that documents for the same assignment share a higher number of words when compared to documents from other assignments. Consequently the pattern mining inside these clusters should yield more meaningful results, allowing for a more accurate classification.

In order to accomplish a good performing clustering, the K-means  method presented by Hartigan~\emph{et al.}~\cite{hartigan79} was used. As K-means tend to be very sensitive to the start condition, the algorithm was ran a hundred times and a majority vote was taken.

This process was accomplished using and R script, which easily allows for the use of various clustering algorithms using the function \texttt{kmeans}. The resulting clustering was tested against the validation software, \texttt{Accuracy.jar}, provided by the instructor, which accused an accuracy of approximately $95\%$

After acquiring a satisfactory clustering result, the input file \texttt{DocumentWords.txt} was split four ways, where each row of the original was copied to a file representing the cluster in which that row was determined to belong to. Each one of these files was subject to its own pattern mining and association rule matching processes.


\subsection{Frequent Pattern Mining}
Documents who share many words are more likely to share other words. In order to be able to efficiently find these commonly shared words, we can consider the \texttt{DocumentWords.txt} as a transactional database, where each document is a transaction, and the words they contain are the items.

The task of finding these frequent item sets was accomplished with the use of an open source Python library originally written by Dagenais~\cite{dagenais10} called pymining. The algorithm used is \emph{Recursive Elimination}, by Borgelt~\emph{et al.}~\cite{borgelt10}~\cite{borgelt05}.

Each cluster file was mined with the minimum support for a frequent item set defined as 20.
%20, 10, 0.45

\subsection{Association Rules Matching}
The association rules was generated based on the frequent patterns found. This process was also generated by a python script using pymining~\cite{dagenais10}. The matching of the rules to the transactions, however, was written by me as a python script.

Let every association rule $r$ be $r: A \rightarrow B$, where $A$ is the set of items that lead to the second set of items $B$. Then the check to see if a rule $r$ matches a transaction $t$, we verify if $(A \subseteq t) \wedge (B \cap t = \emptyset) $.

For each transaction, the matching rules are sorted in descending order of confidence. And the predicted items are selected from these rules in order, until a total of five items are selected.

Finally, the predictions for each cluster were concatenated in a single file so they could be easily read by \texttt{Validation.jar}.

\section{Results}
The results obtained by the aforementioned methodology were compared against a validation set of answers by a software provided by the instructor called \texttt{Validation.jar}, which outputed a score of approximately $34\%$. The table below lists the most important parameters passed to the various functions throughout the methodology in order to ensure the repeatability of the experiment.
\\
  \begin{center}
    \begin{tabular}{llr}
      \toprule
      Function & Parameter & Value\\
      \midrule
      K-means & \texttt{n.clusters} & 4\\
      K-means & \texttt{n.random starts} & 100\\
      Pattern Mining & \texttt{min.support} & 20\\
      Association Rules & \texttt{min.support} & 10\\
      Association Rules & \texttt{min.confidence} & 0.45\\
      \bottomrule
    \end{tabular}
    \\
    \begin{tabular}{cc}
    \toprule
    \multicolumn{2}{c}{Final result}\\
    \midrule
    Average MAP@5 & 0.3449425287356322\\
    \bottomrule
    \end{tabular}
  \end{center}

%\vfill
%\columnbreak

\bibliographystyle{plain}
\bibliography{wmaciel}

%\end{multicols}

\end{document}